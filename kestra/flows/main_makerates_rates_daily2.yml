id: rates_daily2
namespace: makerates

description: |
  Daily Currency Rates Pipeline - Persistent Volume Architecture

  Runs once per day to:
  1. Check API quotas
  2. Extract from Frankfurter/ExchangeRate-API
  3. Transform with dbt (writes to persistent /data/analytics.duckdb)
  4. Upsert validated rates to DynamoDB (reads from persistent /data/analytics.duckdb)

labels:
  project: makerates
  type: incremental
  env: production

inputs:
  - id: execution_date
    type: DATE
    defaults: "{{ trigger.date ?? now() | date('yyyy-MM-dd') }}"
  - id: include_currencylayer
    type: BOOLEAN
    defaults: false
    description: "Run optional 3rd source (CurrencyLayer) - Consumes Quota!"

variables:
  run_mode: "daily"
  # MinIO
  minio_endpoint: "{{ envs.minio_endpoint }}"
  minio_root_user: "{{ envs.minio_root_user }}"
  minio_root_password: "{{ envs.minio_root_password }}"
  # DLT
  dlt_bucket_url: "{{ envs.destination__filesystem__bucket_url }}"
  dlt_layout: "{{ envs.destination__filesystem__layout }}"
  dlt_aws_access_key_id: "{{ envs.destination__filesystem__credentials__aws_access_key_id }}"
  dlt_aws_secret_access_key: "{{ envs.destination__filesystem__credentials__aws_secret_access_key }}"
  # DynamoDB
  dynamodb_endpoint: "{{ envs.dynamodb_endpoint }}"
  ddb_access_key_id: "{{ envs.dynamodb_aws_access_key_id }}"
  ddb_secret_access_key: "{{ envs.dynamodb_aws_secret_access_key }}"
  ddb_default_region: "{{ envs.dynamodb_aws_default_region }}"
  dynamodb_currencies_table_name: "{{ envs.dynamodb_currencies_table_name }}"
  # API Keys
  currencylayer_api_key: "{{ envs.currencylayer_api_key }}"
  exchangerate_api_key: "{{ envs.exchangerate_api_key }}"
  # Buckets
  source_bucket: "s3://{{ envs.bronze_bucket }}"
  target_bucket: "s3://{{ envs.silver_bucket }}/iceberg"  
  # Iceberg
  pyiceberg_catalog_default_uri : "{{envs.pyiceberg_catalog_default_uri}}"
  pyiceberg_catalog_default_type : "{{envs.pyiceberg_catalog_default_type}}"
  pyiceberg_catalog_default_warehouse: "s3://{{ envs.silver_bucket }}/iceberg"
  iceberg_catalog: "{{envs.iceberg_catalog}}"
  iceberg_namespace: "{{envs.iceberg_namespace}}"
  # Source Prefixes
  frankfurter_source_prefix: "{{envs.frankfurter_source_prefix}}"
  currencylayer_source_prefix: "{{envs.currencylayer_source_prefix}}"
  exchangerate_source_prefix: "{{envs.exchangerate_source_prefix}}"
  frankfurter_table_name: "{{envs.frankfurter_table_name}}"
  currencylayer_table_name: "{{envs.currencylayer_table_name }}"
  exchangerate_table_name: "{{envs.exchangerate_table_name}}"
  # DBT
  dbt_silver_bucket: "s3://{{ envs.silver_bucket }}"
  dbt_iceberg_base_path: "iceberg"

tasks:
  # ===== 1. QUOTA CHECK =====
  - id: check_quotas
    type: io.kestra.plugin.docker.Run
    containerImage: makerates-ingestion-base:latest
    pullPolicy: NEVER
    networkMode: makerates-network
    env:
      DYNAMODB_ENDPOINT: "{{ vars.dynamodb_endpoint }}"
      AWS_ACCESS_KEY_ID: "{{ vars.ddb_access_key_id }}"
      AWS_SECRET_ACCESS_KEY: "{{ vars.ddb_secret_access_key }}"
      AWS_DEFAULT_REGION: "{{ vars.ddb_default_region }}"
    commands:
      - quotas_endpoint

  # ===== 2. EXTRACT =====
  - id: extract_parallel
    type: io.kestra.plugin.core.flow.Parallel
    tasks:
      - id: extract_frankfurter
        type: io.kestra.plugin.docker.Run
        containerImage: makerates-ingestion-base:latest
        pullPolicy: NEVER
        networkMode: makerates-network
        runIf: '{{ true }}' 
        env:
          AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          AWS_DEFAULT_REGION: "{{ vars.ddb_default_region }}"
          DYNAMODB_ENDPOINT: "{{ vars.dynamodb_endpoint }}"
          DYNAMODB_AWS_ACCESS_KEY_ID: "{{ vars.ddb_access_key_id }}"
          DYNAMODB_AWS_SECRET_ACCESS_KEY: "{{ vars.ddb_secret_access_key }}"
          MINIO_ENDPOINT: "{{ vars.minio_endpoint }}"
          AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          DESTINATION__FILESYSTEM__BUCKET_URL: "{{ vars.dlt_bucket_url }}"
          DESTINATION__FILESYSTEM__LAYOUT: "{{ vars.dlt_layout }}"
          DESTINATION__FILESYSTEM__EXTRA_PLACEHOLDERS: '{"run_mode": "daily"}'
          DESTINATION__FILESYSTEM__KWARGS__CLIENT_KWARGS__ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          DESTINATION__FILESYSTEM__CREDENTIALS__AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          DESTINATION__FILESYSTEM__CREDENTIALS__AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
        commands:
          - frankfurter_extractor
        timeout: PT5M
        allowFailure: true

      - id: extract_exchangerate
        type: io.kestra.plugin.docker.Run
        containerImage: makerates-ingestion-base:latest
        pullPolicy: NEVER
        networkMode: makerates-network
        runIf: '{{ outputs.check_quotas["vars"]["exchangerate"] }}'
        env:
          AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          AWS_DEFAULT_REGION: "{{ vars.ddb_default_region }}"
          DYNAMODB_ENDPOINT: "{{ vars.dynamodb_endpoint }}"
          DYNAMODB_AWS_ACCESS_KEY_ID: "{{ vars.ddb_access_key_id }}"
          DYNAMODB_AWS_SECRET_ACCESS_KEY: "{{ vars.ddb_secret_access_key }}"
          AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          DESTINATION__FILESYSTEM__BUCKET_URL: "{{ vars.dlt_bucket_url }}"
          DESTINATION__FILESYSTEM__LAYOUT: "{{ vars.dlt_layout }}"
          DESTINATION__FILESYSTEM__EXTRA_PLACEHOLDERS: '{"run_mode": "daily"}'
          DESTINATION__FILESYSTEM__KWARGS__CLIENT_KWARGS__ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          DESTINATION__FILESYSTEM__CREDENTIALS__AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          DESTINATION__FILESYSTEM__CREDENTIALS__AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
        commands:
          - exchangerate_extractor
        timeout: PT5M
        allowFailure: true
      
      - id: extract_currencylayer
        type: io.kestra.plugin.docker.Run
        containerImage: makerates-ingestion-base:latest
        pullPolicy: NEVER
        networkMode: makerates-network
        runIf: '{{ inputs.include_currencylayer or (not outputs.check_quotas.vars.exchangerate) }}'
        env:
          CURRENCYLAYER_API_KEY: "{{ envs.currencylayer_api_key }}"
          AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          MINIO_ENDPOINT: "{{ vars.minio_endpoint }}"
          AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          DESTINATION__FILESYSTEM__BUCKET_URL: "{{ vars.dlt_bucket_url }}"
          DESTINATION__FILESYSTEM__LAYOUT: "{{ vars.dlt_layout }}"
          DESTINATION__FILESYSTEM__EXTRA_PLACEHOLDERS: '{"run_mode": "daily"}'
          DESTINATION__FILESYSTEM__KWARGS__CLIENT_KWARGS__ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          DESTINATION__FILESYSTEM__CREDENTIALS__AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          DESTINATION__FILESYSTEM__CREDENTIALS__AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          DYNAMODB_ENDPOINT: "{{ vars.dynamodb_endpoint }}"
          DYNAMODB_AWS_ACCESS_KEY_ID: "{{ vars.ddb_access_key_id }}"
          DYNAMODB_AWS_SECRET_ACCESS_KEY: "{{ vars.ddb_secret_access_key }}"
        commands:
          - currencylayer_extractor
        timeout: PT5M
        allowFailure: true

  # ===== 2a. INITIALIZE ICEBERG CATALOG =====
  - id: init_iceberg_catalog
    type: io.kestra.plugin.docker.Run
    containerImage: makerates-ingestion-base:latest
    pullPolicy: NEVER
    networkMode: makerates-network
    env:
      PYICEBERG_CATALOG__DEFAULT__TYPE: "{{ vars.pyiceberg_catalog_default_type }}"
      PYICEBERG_CATALOG__DEFAULT__URI: "{{ vars.pyiceberg_catalog_default_uri }}"
      PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: "{{ vars.minio_endpoint }}"
      PYICEBERG_CATALOG__DEFAULT__WAREHOUSE: "{{ vars.pyiceberg_catalog_default_warehouse }}"
      AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
      AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
      AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"
      ICEBERG_CATALOG: "{{ vars.iceberg_catalog }}"
      ICEBERG_NAMESPACE: "{{ vars.iceberg_namespace }}"
    commands:
      - init_iceberg_catalog

  # ===== 2b. COMPACT TO ICEBERG =====
  - id: create_iceberg_sources
    type: io.kestra.plugin.core.flow.Parallel
    tasks:
      - id: load_frankfurter_to_iceberg
        dependsOn:
          - extract_frankfurter
        type: io.kestra.plugin.docker.Run
        containerImage: makerates-ingestion-base:latest
        pullPolicy: NEVER
        networkMode: makerates-network
        env:
          AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          AWS_REGION: "{{ vars.ddb_default_region }}"
          AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          SOURCE_BUCKET: "{{ vars.source_bucket }}"
          SOURCE_PREFIX: "{{ vars.frankfurter_source_prefix }}"
          TARGET_BUCKET: "{{ vars.target_bucket }}"
          TABLE_NAME: "{{ vars.frankfurter_table_name }}"
          ICEBERG_CATALOG: "{{ vars.iceberg_catalog }}"
          ICEBERG_NAMESPACE: "{{ vars.iceberg_namespace }}"
          PYICEBERG_CATALOG__DEFAULT__TYPE: "{{ vars.pyiceberg_catalog_default_type }}"
          PYICEBERG_CATALOG__DEFAULT__URI: "{{ vars.pyiceberg_catalog_default_uri }}"
          PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: "{{ vars.minio_endpoint }}"
          PYICEBERG_CATALOG__DEFAULT__WAREHOUSE: "{{ vars.pyiceberg_catalog_default_warehouse }}"
        commands:
          - iceberg_loader 
          - "--start-date={{ inputs.execution_date }}"
          - "--mode={{ vars.run_mode }}"

      - id: load_exchangerate_to_iceberg
        dependsOn:
          - extract_exchangerate
        type: io.kestra.plugin.docker.Run
        containerImage: makerates-ingestion-base:latest
        pullPolicy: NEVER
        networkMode: makerates-network
        runIf: '{{ outputs.check_quotas["vars"]["exchangerate"] }}'
        env:
          AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          AWS_REGION: "{{ vars.ddb_default_region }}"
          AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          SOURCE_BUCKET: "{{ vars.source_bucket }}"
          SOURCE_PREFIX: "{{ vars.exchangerate_source_prefix }}"
          TARGET_BUCKET: "{{ vars.pyiceberg_catalog_default_warehouse }}"
          TABLE_NAME: "{{ vars.exchangerate_table_name }}"
          ICEBERG_CATALOG: "{{ vars.iceberg_catalog }}"
          ICEBERG_NAMESPACE: "{{ vars.iceberg_namespace }}"
          PYICEBERG_CATALOG__DEFAULT__TYPE: "{{vars.pyiceberg_catalog_default_type}}"
          PYICEBERG_CATALOG__DEFAULT__URI: "{{vars.pyiceberg_catalog_default_uri}}"
          PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: "{{ vars.minio_endpoint }}"
          PYICEBERG_CATALOG__DEFAULT__WAREHOUSE: "{{ vars.pyiceberg_catalog_default_warehouse }}"
        commands:
          - iceberg_loader 
          - "--start-date={{ inputs.execution_date }}"
          - "--mode={{ vars.run_mode }}"
      
      - id: load_currencylayer_to_iceberg
        dependsOn:
          - extract_currencylayer
        type: io.kestra.plugin.docker.Run
        containerImage: makerates-ingestion-base:latest
        pullPolicy: NEVER
        networkMode: makerates-network
        runIf: '{{ inputs.include_currencylayer or (not outputs.check_quotas.vars.exchangerate) }}'
        env:
          AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          AWS_REGION: "{{ vars.ddb_default_region }}"
          AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          SOURCE_BUCKET: "{{ vars.source_bucket }}"
          SOURCE_PREFIX: "{{ vars.currencylayer_source_prefix }}"
          TARGET_BUCKET: "{{ vars.pyiceberg_catalog_default_warehouse }}"
          TABLE_NAME: "{{ vars.currencylayer_table_name }}"
          ICEBERG_CATALOG: "{{ vars.iceberg_catalog }}"
          ICEBERG_NAMESPACE: "{{ vars.iceberg_namespace }}"
          PYICEBERG_CATALOG__DEFAULT__TYPE: "{{vars.pyiceberg_catalog_default_type}}"
          PYICEBERG_CATALOG__DEFAULT__URI: "{{vars.pyiceberg_catalog_default_uri}}"
          PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: "{{ vars.minio_endpoint }}"
          PYICEBERG_CATALOG__DEFAULT__WAREHOUSE: "{{ vars.pyiceberg_catalog_default_warehouse }}"
        commands: 
          - iceberg_loader 
          - "--start-date={{ inputs.execution_date }}"
          - "--mode={{ vars.run_mode }}"

  # ===== 3. TRANSFORM (dbt) =====
  - id: git_sync
    type: io.kestra.plugin.git.SyncNamespaceFiles
    url: https://github.com/ManosCoffee/Makerates
    branch: main
    namespace: makerates
    gitDirectory: dbt_project
    
  # - id: fetch_metadata_locations
  #   type: io.kestra.plugin.scripts.shell.Commands
  #   containerImage: amazon/aws-cli
  #   taskRunner:
  #     type: io.kestra.plugin.scripts.runner.docker.Docker
  #     networkMode: makerates-network
  #   env:
  #     AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
  #     AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
  #     AWS_DEFAULT_REGION: "{{ vars.ddb_default_region }}"
  #     # AWS CLI uses AWS_ENDPOINT_URL in newer versions, providing both for safety
  #     AWS_ENDPOINT_URL: "{{ vars.dynamodb_endpoint }}"
  #     AWS_ENDPOINT_URL_DYNAMODB: "{{ vars.dynamodb_endpoint }}"
  #   commands:
  #     # Fetch metadata location using AWS CLI with JMESPath query
  #     # Output format: ::{"outputs":{"key":"value"}}:: for Kestra to parse
  #     - |
  #       LOCATION=$(aws dynamodb get-item \
  #         --table-name iceberg_metadata \
  #         --key '{"table_name": {"S": "frankfurter_rates"}}' \
  #         --query 'Item.metadata_location.S' \
  #         --output text)
        
  #       echo "::{\"outputs\":{\"frankfurter_metadata_location\":\"$LOCATION\"}}::"
    
  - id: dbt_validation_and_transform
    type: io.kestra.plugin.dbt.cli.DbtCLI
    containerImage: makerates-ingestion-base:latest
    allowWarning: true
    namespaceFiles:
      enabled: true
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      networkMode: makerates-network
      # volumes:
        # - /Users/manos/dev/makerates/data:/data
    outputFiles:
      - analytics.duckdb
    env:
      AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
      AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
      MINIO_ENDPOINT: "{{ vars.minio_endpoint }}"
      MINIO_ROOT_USER: "{{ vars.minio_root_user }}"
      MINIO_ROOT_PASSWORD: "{{ vars.minio_root_password }}"
      DUCKDB_PATH: "analytics.duckdb" # Writes to working dir for outputFiles
      DBT_SILVER_BUCKET: "{{ vars.dbt_silver_bucket }}"
      DBT_ICEBERG_BASE_PATH: "{{ vars.dbt_iceberg_base_path }}"
      EXECUTION_DATE: "{{ inputs.execution_date }}"
      PIPELINE_MODE: "{{ vars.run_mode }}"
    commands:
      - dbt deps
      - "dbt build --vars '{\"frankfurter_metadata_location\": \"{{ outputs.fetch_metadata_locations.vars.frankfurter_metadata_location }}\"}'"
      - pwd
      - ls -lah .
      - find . -name "*.duckdb" -o -name "*.db"
      # - cp analytics.duckdb /data/analytics_copied.duckdb

  - id: upload_ddb_file
    type: io.kestra.plugin.minio.Upload
    namespaceFiles:
      enabled: true
    region: "{{ envs.dynamodb_aws_default_region }}"
    endpoint: "{{ envs.minio_endpoint }}"  
    accessKeyId: "{{ envs.destination__filesystem__credentials__aws_access_key_id }}"
    secretKeyId: "{{ envs.destination__filesystem__credentials__aws_secret_access_key }}"
    from: "{{ outputs.dbt_validation_and_transform.outputFiles['analytics.duckdb'] }}" #"{{ outputs.dbt_validation_and_transform.outputFiles['analytics.duckdb'] }}"
    bucket: "analytics-bucket"
    key: "analytics.duckdb"
         


  # ===== 4. UPSERT TO DYNAMODB =====
  - id: sync_to_dynamodb
    type: io.kestra.plugin.docker.Run
    namespaceFiles:
      enabled: true
    dependsOn:
      - dbt_build
    containerImage: makerates-ingestion-base:latest
    pullPolicy: NEVER
    networkMode: makerates-network
    inputFiles:
      analytics.duckdb: "{{ outputs.dbt_validation_and_transform.outputFiles['analytics.duckdb'] }}"
    env:
      AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
      AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
      AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"
      AWS_REGION: "{{ vars.ddb_default_region }}"
      DYNAMODB_TABLE_NAME: "{{ vars.dynamodb_currencies_table_name }}"
      DYNAMODB_ENDPOINT_URL: "{{ vars.dynamodb_endpoint }}"
      DYNAMODB_REGION: "{{ vars.ddb_default_region }}"
      DUCKDB_PATH: "analytics.duckdb" # Reads from Kestra inputFiles
      MODE: "{{ vars.run_mode }}"
    workingDir: /app
    commands:
      - sync_gold_to_dynamodb # Uses python -m entrypoint, PYTHONPATH=/app/src
      # - sync_gold_to_dynamodb
    timeout: PT5M
    retry:
      type: constant
      interval: PT30S
      maxAttempt: 3


triggers:
  - id: daily_schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: "0 6 * * *"
    description: Daily at 6 AM UTC
    inputs:
      execution_date: "{{ trigger.date | date('yyyy-MM-dd') }}"

errors:
  - id: backfill_error
    type: io.kestra.plugin.core.log.Log
    message: I'm failing {{ errorLogs()[0]['taskId'] }} 
    level: ERROR
