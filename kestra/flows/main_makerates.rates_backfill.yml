id: rates_backfill
namespace: makerates

description: |
  Currency Rates Backfill - Historical Data Load

  Loads historical rates for a date range in BATCH mode.
  Parallel extraction from Frankfurter and CurrencyLayer.

  Quota-aware: Respects API limits.

labels:
  project: makerates
  type: backfill
  env: production

inputs:
  - id: start_date
    type: DATE
    required: true
    description: Start date (YYYY-MM-DD)

  - id: end_date
    type: DATE
    required: true
    description: End date (YYYY-MM-DD, inclusive)

variables:
  #run mode = backfill
  run_mode: "backfill"
  # MinIO specific
  minio_endpoint: "{{ envs.minio_endpoint }}"
  minio_root_user: "{{ envs.minio_root_user }}"
  minio_root_password: "{{ envs.minio_root_password }}"
  # DLT Reusable Vars
  dlt_bucket_url: "{{ envs.destination__filesystem__bucket_url }}"
  dlt_layout: "{{ envs.destination__filesystem__layout }}"
  dlt_extra_placeholders: "{{ envs.destination__filesystem__extra_placeholders }}"
  dlt_aws_access_key_id: "{{ envs.destination__filesystem__credentials__aws_access_key_id }}"
  dlt_aws_secret_access_key: "{{ envs.destination__filesystem__credentials__aws_secret_access_key }}"
  # DynamoDB Specific
  dynamodb_endpoint: "{{ envs.dynamodb_endpoint }}"
  ddb_access_key_id: "{{ envs.dynamodb_aws_access_key_id }}"
  ddb_secret_access_key: "{{ envs.dynamodb_aws_secret_access_key }}"
  ddb_default_region: "{{ envs.dynamodb_aws_default_region }}"
  # api keys
  currencylayer_api_key: "{{ envs.currencylayer_api_key }}"
  # BUCKETS
  source_bucket: "s3://{{ envs.bronze_bucket }}"
  target_bucket: "s3://{{ envs.silver_bucket }}/iceberg"  
  # ICEBERG
  pyiceberg_catalog_default_uri : "{{envs.pyiceberg_catalog_default_uri}}"
  pyiceberg_catalog_default_type : "{{envs.pyiceberg_catalog_default_type}}"
  pyiceberg_catalog_default_warehouse: "s3://{{ envs.silver_bucket }}/iceberg"
  frankfurter_source_prefix: "{{envs.frankfurter_source_prefix}}"
  currencylayer_source_prefix: "{{envs.currencylayer_source_prefix}}"
  frankfurter_table_name: "{{envs.frankfurter_table_name}}"
  currencylayer_table_name: "{{envs.currencylayer_table_name }}"
  iceberg_catalog: "{{envs.iceberg_catalog}}"
  iceberg_namespace: "{{envs.iceberg_namespace}}"
  # dbt variables
  dbt_silver_bucket: "s3://{{ envs.silver_bucket }}"
  dbt_iceberg_base_path: "iceberg"



tasks:

  # ===== INIT QUOTAS =====
  - id: check_quotas
    type: io.kestra.plugin.docker.Run
    containerImage: makerates-ingestion-base:latest
    pullPolicy: NEVER
    networkMode: makerates-network
    env:
      DYNAMODB_ENDPOINT: "{{ vars.dynamodb_endpoint }}"
      DYNAMODB_AWS_ACCESS_KEY_ID: "{{ vars.ddb_access_key_id }}"
      DYNAMODB_AWS_SECRET_ACCESS_KEY: "{{ vars.ddb_secret_access_key }}"
      DYNAMODB_AWS_DEFAULT_REGION: "{{ vars.ddb_default_region }}"
    commands:
      - quotas_endpoint

  - id: backfill_parallel
    type: io.kestra.plugin.core.flow.Parallel
    tasks:
      - id: extract_frankfurter
        type: io.kestra.plugin.docker.Run
        containerImage: makerates-ingestion-base:latest
        pullPolicy: NEVER
        networkMode: makerates-network
        env:
          # Global
          AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          AWS_DEFAULT_REGION: "{{ vars.ddb_default_region }}"
          # DynamoDB
          DYNAMODB_ENDPOINT: "{{ vars.dynamodb_endpoint }}"
          DYNAMODB_AWS_ACCESS_KEY_ID: "{{ vars.ddb_access_key_id }}"
          DYNAMODB_AWS_SECRET_ACCESS_KEY: "{{ vars.ddb_secret_access_key }}"
          # MinIO & DLT specific
          MINIO_ENDPOINT: "{{ vars.minio_endpoint }}"
          AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}" # Global override for boto3/s3fs
          DESTINATION__FILESYSTEM__BUCKET_URL: "{{ vars.dlt_bucket_url }}"
          DESTINATION__FILESYSTEM__LAYOUT: "{{ vars.dlt_layout }}"
          DESTINATION__FILESYSTEM__EXTRA_PLACEHOLDERS: '{"run_mode": "backfill"}'
          DESTINATION__FILESYSTEM__KWARGS__CLIENT_KWARGS__ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          DESTINATION__FILESYSTEM__CREDENTIALS__AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          DESTINATION__FILESYSTEM__CREDENTIALS__AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          
        commands:
          - frankfurter_extractor 
          - "--start-date={{ inputs.start_date }}"
          - "--end-date={{ inputs.end_date }}"
      
      - id: extract_currencylayer
        type: io.kestra.plugin.docker.Run
        containerImage: makerates-ingestion-base:latest
        pullPolicy: NEVER
        networkMode: makerates-network
        runIf: '{{ outputs.check_quotas["vars"]["currencylayer"] }}'
        env:
          CURRENCYLAYER_API_KEY: "{{ vars.currencylayer_api_key }}"
          # Global
          AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          AWS_DEFAULT_REGION: "{{ vars.ddb_default_region }}"
          # DynamoDB
          DYNAMODB_ENDPOINT: "{{ vars.dynamodb_endpoint }}"
          DYNAMODB_AWS_ACCESS_KEY_ID: "{{ vars.ddb_access_key_id }}"
          DYNAMODB_AWS_SECRET_ACCESS_KEY: "{{ vars.ddb_secret_access_key }}"
          # MinIO & DLT specific
          MINIO_ENDPOINT: "{{ vars.minio_endpoint }}"
          AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}" # Global override for boto3/s3fs
          
          DESTINATION__FILESYSTEM__BUCKET_URL: "{{ vars.dlt_bucket_url }}"
          DESTINATION__FILESYSTEM__LAYOUT: "{{ vars.dlt_layout }}"
          DESTINATION__FILESYSTEM__EXTRA_PLACEHOLDERS: '{"run_mode": "backfill"}'
          
          # Explicitly set nested config to avoid JSON parsing issues
          DESTINATION__FILESYSTEM__KWARGS__CLIENT_KWARGS__ENDPOINT_URL: "{{ vars.minio_endpoint }}"
          
          DESTINATION__FILESYSTEM__CREDENTIALS__AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          DESTINATION__FILESYSTEM__CREDENTIALS__AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
        commands:
          - currencylayer_extractor
          - "--start-date={{ inputs.start_date }}"
          - "--end-date={{ inputs.end_date }}"
        allowFailure: true 
  # ===== 2a. INITIALIZE ICEBERG CATALOG =====
  - id: init_iceberg_catalog
    description: "Initialize Iceberg catalog (prevents race condition)"
    type: io.kestra.plugin.docker.Run
    allowWarning: true
    containerImage: makerates-ingestion-base:latest
    pullPolicy: NEVER
    networkMode: makerates-network
    env:
      PYICEBERG_CATALOG__DEFAULT__TYPE: "{{ vars.pyiceberg_catalog_default_type }}"
      PYICEBERG_CATALOG__DEFAULT__URI: "{{ vars.pyiceberg_catalog_default_uri }}"
      PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: "{{ vars.minio_endpoint }}"
      PYICEBERG_CATALOG__DEFAULT__WAREHOUSE: "{{ vars.pyiceberg_catalog_default_warehouse }}"
      AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
      AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
      AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"
      ICEBERG_CATALOG: "{{ vars.iceberg_catalog }}"
      ICEBERG_NAMESPACE: "{{ vars.iceberg_namespace }}"
    commands:
      - init_iceberg_catalog

  # ===== 2b. LOAD TO ICEBERG (Bronze -> Silver) =====
  - id: create_iceberg_sources
    description: "Compact backfill JSONL into optimized Iceberg tables"
    type: io.kestra.plugin.core.flow.Parallel
    allowWarning: true
    tasks:
      - id: load_frankfurter_to_iceberg
        dependsOn:
          - extract_frankfurter
        allowWarning: true
        type: io.kestra.plugin.docker.Run
        containerImage: makerates-ingestion-base:latest
        pullPolicy: NEVER
        networkMode: makerates-network
        env:
          # AWS creds for MinIO S3 (ICEBERG)
          AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          AWS_REGION: "{{ vars.ddb_default_region }}"
          AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"

          # AWS creds for DynamoDB (ICEBERG)
          DYNAMODB_ENDPOINT: "{{ vars.dynamodb_endpoint }}"
          DYNAMODB_AWS_ACCESS_KEY_ID: "{{ vars.ddb_access_key_id }}"
          DYNAMODB_AWS_SECRET_ACCESS_KEY: "{{ vars.ddb_secret_access_key }}"

          # --- Iceberg App Params ---
          SOURCE_BUCKET: "{{ vars.source_bucket }}"
          SOURCE_PREFIX: "{{ vars.frankfurter_source_prefix }}"
          TARGET_BUCKET: "{{ vars.target_bucket }}"
          TABLE_NAME: "{{ vars.frankfurter_table_name }}"
          ICEBERG_CATALOG: "{{ vars.iceberg_catalog }}"
          ICEBERG_NAMESPACE: "{{ vars.iceberg_namespace }}"

          # --- Catalog Configuration (Postgres) ---
          PYICEBERG_CATALOG__DEFAULT__TYPE: "{{ vars.pyiceberg_catalog_default_type }}"
          PYICEBERG_CATALOG__DEFAULT__URI: "{{ vars.pyiceberg_catalog_default_uri }}"
          PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: "{{ vars.minio_endpoint }}"
          PYICEBERG_CATALOG__DEFAULT__WAREHOUSE: "{{ vars.pyiceberg_catalog_default_warehouse }}"
        commands:
          - iceberg_loader
          - "--start-date={{ inputs.start_date }}"
          - "--end-date={{ inputs.end_date }}"
          - "--mode={{vars.run_mode}}"

      - id: load_currencylayer_to_iceberg
        dependsOn:
          - extract_currencylayer
        allowWarning: true
        type: io.kestra.plugin.docker.Run
        containerImage: makerates-ingestion-base:latest
        pullPolicy: NEVER
        networkMode: makerates-network
        env:
          # AWS creds for MinIO S3 (ICEBERG)
          AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
          AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
          AWS_REGION: "{{ vars.ddb_default_region }}"
          AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"

          # AWS creds for DynamoDB (ICEBERG)
          DYNAMODB_ENDPOINT: "{{ vars.dynamodb_endpoint }}"
          DYNAMODB_AWS_ACCESS_KEY_ID: "{{ vars.ddb_access_key_id }}"
          DYNAMODB_AWS_SECRET_ACCESS_KEY: "{{ vars.ddb_secret_access_key }}"

          # --- Iceberg App Params ---
          SOURCE_BUCKET: "{{ vars.source_bucket }}"
          SOURCE_PREFIX: "{{ vars.currencylayer_source_prefix }}"
          TARGET_BUCKET: "{{ vars.pyiceberg_catalog_default_warehouse }}" #"s3://{{ vars.silver_bucket }}/iceberg"
          TABLE_NAME: "{{ vars.currencylayer_table_name }}"
          ICEBERG_CATALOG: "{{ vars.iceberg_catalog }}"
          ICEBERG_NAMESPACE: "{{ vars.iceberg_namespace }}"

          # --- Catalog Configuration (Postgres) ---
          PYICEBERG_CATALOG__DEFAULT__TYPE: "{{vars.pyiceberg_catalog_default_type}}"
          PYICEBERG_CATALOG__DEFAULT__URI: "{{vars.pyiceberg_catalog_default_uri}}"
          PYICEBERG_CATALOG__DEFAULT__S3__ENDPOINT: "{{ vars.minio_endpoint }}"
          PYICEBERG_CATALOG__DEFAULT__WAREHOUSE: "{{ vars.pyiceberg_catalog_default_warehouse }}"
        commands:
          - iceberg_loader
          - "--start-date={{ inputs.start_date }}"
          - "--end-date={{ inputs.end_date }}"
          - "--mode={{vars.run_mode}}"

  # ===== 2c. FETCH METADATA LOCATIONS FROM DYNAMODB =====
  - id: fetch_iceberg_metadata_paths
    type: io.kestra.plugin.scripts.python.Script
    containerImage: python:3.12-slim
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      networkMode: makerates-network
    dependsOn:
      - create_iceberg_sources
    beforeCommands:
      - pip install boto3 kestra
    env:
      AWS_ACCESS_KEY_ID: "{{ vars.ddb_access_key_id }}"
      AWS_SECRET_ACCESS_KEY: "{{ vars.ddb_secret_access_key }}"
      AWS_DEFAULT_REGION: "{{ vars.ddb_default_region }}"
      DYNAMODB_ENDPOINT: "{{ vars.dynamodb_endpoint }}"
    script: |
      import boto3
      import os
      from kestra import Kestra

      # Initialize DynamoDB client
      ddb = boto3.client(
          'dynamodb',
          endpoint_url=os.environ['DYNAMODB_ENDPOINT'],
          region_name=os.environ['AWS_DEFAULT_REGION'],
          aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],
          aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']
      )

      # Fetch metadata for each table (backfill only uses Frankfurter & CurrencyLayer)
      tables = {
          'frankfurter_rates': 'frankfurter_metadata',
          'currencylayer_rates': 'currencylayer_metadata'
      }

      outputs = {}
      for table_name, output_key in tables.items():
          try:
              response = ddb.get_item(
                  TableName='iceberg_metadata',
                  Key={'table_name': {'S': table_name}}
              )
              if 'Item' in response and 'metadata_location' in response['Item']:
                  metadata_location = response['Item']['metadata_location']['S']
                  outputs[output_key] = metadata_location
                  print(f"✅ {table_name}: {metadata_location}")
              else:
                  outputs[output_key] = ""
                  print(f"⚠️  {table_name}: No metadata found")
          except Exception as e:
              outputs[output_key] = ""
              print(f"❌ {table_name}: Error - {e}")

      # Output using Kestra SDK
      Kestra.outputs(outputs)

  # ===== 3. SYNC DBT PROJECT FROM GIT =====
  - id: sync
    type: io.kestra.plugin.git.SyncNamespaceFiles
    disabled: false
    url: https://github.com/ManosCoffee/Makerates
    branch: main
    namespace: makerates
    gitDirectory: dbt_project
    dryRun: false

  # ===== 4. TRANSFORM (dbt) via Namespace Files =====
  - id: dbt_validation_and_transform
    type: io.kestra.plugin.dbt.cli.DbtCLI
    allowWarning: true
    containerImage: makerates-ingestion-base:latest
    dependsOn:
      - fetch_iceberg_metadata_paths
    namespaceFiles:
      enabled: true
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      networkMode: makerates-network
      volumes:
        - "{{ envs.host_pwd }}/data:/data"
    outputFiles:
      - /data/analytics.duckdb
    env:
      AWS_ACCESS_KEY_ID: "{{ vars.dlt_aws_access_key_id }}"
      AWS_SECRET_ACCESS_KEY: "{{ vars.dlt_aws_secret_access_key }}"
      AWS_ENDPOINT_URL: "{{ vars.minio_endpoint }}"
      AWS_REGION: "{{ vars.ddb_default_region }}"
      MINIO_ENDPOINT: "{{ vars.minio_endpoint }}"
      MINIO_ROOT_USER: "{{ vars.minio_root_user }}"
      MINIO_ROOT_PASSWORD: "{{ vars.minio_root_password }}"
      DUCKDB_PATH: "/data/analytics.duckdb"
      DBT_SILVER_BUCKET: "{{ vars.dbt_silver_bucket }}"
      DBT_ICEBERG_BASE_PATH: "{{ vars.dbt_iceberg_base_path }}"
      EXECUTION_DATE: "{{ inputs.start_date }}"
      PIPELINE_MODE: "backfill"
      # Iceberg metadata locations from DynamoDB (for dbt macro optimization)
      FRANKFURTER_METADATA_LOCATION: "{{ outputs.fetch_iceberg_metadata_paths.outputs.frankfurter_metadata | default('') }}"
      CURRENCYLAYER_METADATA_LOCATION: "{{ outputs.fetch_iceberg_metadata_paths.outputs.currencylayer_metadata | default('') }}"
    commands:
      - dbt deps
      - dbt build
      
errors:
  - id: backfill_error
    type: io.kestra.plugin.core.log.Log
    message: I'm failing {{ errorLogs()[0]['taskId'] }} # Because errorLogs() is an array, the first taskId to fail is retrieved.
    level: ERROR